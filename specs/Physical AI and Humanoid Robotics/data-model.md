# Data Model: Physical AI and Humanoid Robotics

**Date**: 2025-12-05 | **Spec**: ../specs/Physical AI and Humanoid Robotics/spec.md | **Plan**: ../specs/Physical AI and Humanoid Robotics/plan.md

## Summary

This document outlines the conceptual data model for the "Physical AI and Humanoid Robotics" book, focusing on the key entities, data types, and information flows within the robotics and AI systems discussed. It serves as a guide for understanding the structure and interaction of data in simulations and AI pipelines.

## Simulation Elements

- **Gazebo Models**: 3D representations of robots and environments (`.sdf`, `.urdf` files).
  - **Entities**: Links, Joints, Sensors, Actuators, Collisions, Visuals.
  - **Properties**: Mass, inertia, friction, position, orientation.
- **Unity Assets**: High-fidelity 3D models, textures, materials, animations for visual representation and interaction.
- **Physics Properties**: Gravity, friction coefficients, collision parameters, joint limits, motor forces.
- **Sensor Data**: Raw and processed data from simulated sensors.
  - **LiDAR**: Point clouds (e.g., `sensor_msgs/msg/PointCloud2`).
  - **Depth Cameras**: Depth images (`sensor_msgs/msg/Image`), point clouds.
  - **IMUs**: Orientation, angular velocity, linear acceleration (`sensor_msgs/msg/Imu`).
- **Simulation State**: Current positions, velocities, accelerations of all entities; time.

## AI Perception Data

- **Camera Images**: RGB images (`sensor_msgs/msg/Image`) from simulated cameras.
- **LiDAR Scans**: Processed point cloud data used for obstacle detection and mapping.
- **IMU Data**: Filtered inertial data for robot pose estimation and stabilization.
- **VSLAM Maps**: Occupancy grids or point cloud maps (`nav_msgs/msg/OccupancyGrid`, `sensor_msgs/msg/PointCloud2`) generated by visual simultaneous localization and mapping.
- **Synthetic Data**: Artificially generated datasets (images, point clouds, labels) for training AI models, often with ground truth annotations (e.g., from NVIDIA Isaac Sim Replicator).
- **Object Detections/Identifications**: Bounding boxes, class labels, confidence scores for objects in the environment.

## AI Control/Planning Data

- **ROS 2 Messages**: Standard ROS 2 message types for communication between nodes.
  - **Topics**: Data streams for sensor readings, command velocities, robot states (`geometry_msgs/msg/Twist`, `sensor_msgs/msg/JointState`).
  - **Services**: Request/response for specific actions (e.g., `std_srvs/srv/Trigger`).
  - **Actions**: Long-running goal-oriented tasks (e.g., `nav2_msgs/action/NavigateToPose`).
- **Nav2 Path Plans**: Global and local paths (`nav_msgs/msg/Path`) for robot navigation.
- **Cognitive LLM Outputs**: High-level action plans or sequences of sub-goals generated by large language models (e.g., "pick up the red cube," broken down into sub-commands).
- **Robot Commands**: Low-level motor commands (joint angles, velocities, forces) derived from AI planning.

## Humanoid Interaction

- **Voice Commands**: Audio input from users, converted to text.
  - **Whisper Output**: Transcribed text from speech (e.g., `std_msgs/msg/String`).
- **Conversational AI Context**: Internal state of the conversational agent, tracking dialogue history, user intent, and current task progress.
- **Visual Feedback**: Rendered scenes, object highlights, robot responses displayed to the user.

## Relationships/Flows (ROS 2 graphs, simulation loops, AI pipelines)

### ROS 2 Graph (Conceptual)

```mermaid
graph TD
    A[Voice Command] --> B{OpenAI Whisper}
    B --> C[Text Command]
    C --> D{Cognitive LLM Planning}
    D --> E[Action Plan (high-level)]
    E --> F{ROS 2 Action Client}
    F --> G[Robot State (JointState, Pose)]
    G --> H{Nav2 Stack}
    H --> I[Path Plan]
    I --> J{Motion Controller}
    J --> K[Low-level Robot Commands]
    K --> L[Simulated Humanoid (Gazebo/Isaac Sim)]
    L --> M[Sensor Data]
    M --> N{Perception Pipeline (Isaac ROS)}
    N --> O[Object Detections, VSLAM Maps]
    O --> D
```

### Simulation Loop (Gazebo/Isaac Sim)

- **Input**: Robot commands, environmental parameters.
- **Process**: Physics engine updates (collisions, gravity, dynamics), sensor data generation, visual rendering.
- **Output**: Updated simulation state, sensor data streams.

### AI Pipelines (e.g., VLA Pipeline)

- **Voice-to-Action**:
    1. Voice command captured.
    2. Transcribed to text via OpenAI Whisper.
    3. Text input to Cognitive LLM for high-level planning.
    4. LLM output (action plan) converted to ROS 2 actions.
- **Perception-to-Planning**:
    1. Sensor data (camera, LiDAR) processed by Isaac ROS for object detection and VSLAM.
    2. Perception outputs feed into Nav2 for path planning and/or Cognitive LLM for decision-making.
- **Control Loop**:
    1. High-level action plan decomposed into low-level robot commands.
    2. Commands sent to simulated robot via ROS 2.
    3. Robot state and sensor feedback used to close the loop and update AI models.
