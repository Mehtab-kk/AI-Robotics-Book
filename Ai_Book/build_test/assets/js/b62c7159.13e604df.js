"use strict";(globalThis.webpackChunkai_book=globalThis.webpackChunkai_book||[]).push([[168],{2713:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-3-ai-robot-brain/chapter-08-mini-project","title":"chapter-8 Mini-Project: AI-Robot Brain Test","description":"Overview","source":"@site/docs/module-3-ai-robot-brain/chapter-08-mini-project.md","sourceDirName":"module-3-ai-robot-brain","slug":"/module-3-ai-robot-brain/chapter-08-mini-project","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-3-ai-robot-brain/chapter-08-mini-project","draft":false,"unlisted":false,"editUrl":"https://github.com/Mehtab-kk/Physical-AI-Humanoid-Robotics-Book/edit/master/docs/module-3-ai-robot-brain/chapter-08-mini-project.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"id":"chapter-08-mini-project","title":"chapter-8 Mini-Project: AI-Robot Brain Test","sidebar_position":8},"sidebar":"tutorialSidebar","previous":{"title":"chapter-7 AI Action Planning","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-3-ai-robot-brain/chapter-07-ai-action-planning"},"next":{"title":"Chapter-1 Introduction to Vision-Language-Action Systems","permalink":"/Physical-AI-Humanoid-Robotics-Book/docs/module-4-vla/chapter-1-introduction-to-vla"}}');var a=i(4848),o=i(8453);const s={id:"chapter-08-mini-project",title:"chapter-8 Mini-Project: AI-Robot Brain Test",sidebar_position:8},r="Mini-Project: AI-Robot Brain Test",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Project Objective",id:"project-objective",level:2},{value:"System Architecture",id:"system-architecture",level:2},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"Step 1: Environment Setup",id:"step-1-environment-setup",level:3},{value:"Step 2: Sensor Integration System",id:"step-2-sensor-integration-system",level:3},{value:"Step 3: VSLAM Integration",id:"step-3-vslam-integration",level:3},{value:"Step 4: Nav2 Path Planning",id:"step-4-nav2-path-planning",level:3},{value:"Step 5: AI Action Planning",id:"step-5-ai-action-planning",level:3},{value:"Step 6: Complete Integration Loop",id:"step-6-complete-integration-loop",level:3},{value:"Testing Scenarios",id:"testing-scenarios",level:2},{value:"Scenario 1: Simple Navigation",id:"scenario-1-simple-navigation",level:3},{value:"Scenario 2: Obstacle Avoidance",id:"scenario-2-obstacle-avoidance",level:3},{value:"Scenario 3: Dynamic Environment",id:"scenario-3-dynamic-environment",level:3},{value:"Scenario 4: Complex Navigation",id:"scenario-4-complex-navigation",level:3},{value:"Assessment Criteria",id:"assessment-criteria",level:2},{value:"Performance Metrics",id:"performance-metrics",level:3},{value:"System Integration",id:"system-integration",level:3},{value:"AI Decision Quality",id:"ai-decision-quality",level:3},{value:"Mini-Exercise",id:"mini-exercise",level:2},{value:"Summary",id:"summary",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"mini-project-ai-robot-brain-test",children:"Mini-Project: AI-Robot Brain Test"})}),"\n",(0,a.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(e.p,{children:"In this final chapter, we'll integrate all the concepts from Module 3 to create a complete AI-robot brain system. This mini-project combines Isaac Sim, Isaac ROS, VSLAM, Nav2, sensor integration, and AI action planning into a unified system for a humanoid robot."}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"AI-Robot Brain (AI Robot ka dimag):"})," The integrated system that combines perception, planning, and action for intelligent robot behavior."]}),"\n",(0,a.jsx)(e.h2,{id:"project-objective",children:"Project Objective"}),"\n",(0,a.jsx)(e.p,{children:"Create an AI-powered humanoid robot that can:"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Navigate autonomously in a simulated environment"}),"\n",(0,a.jsx)(e.li,{children:"Use VSLAM for localization and mapping"}),"\n",(0,a.jsx)(e.li,{children:"Plan paths using Nav2 to reach specified goals"}),"\n",(0,a.jsx)(e.li,{children:"Integrate multiple sensors for perception"}),"\n",(0,a.jsx)(e.li,{children:"Make intelligent decisions using AI action planning"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,a.jsx)(e.p,{children:"The complete AI-robot brain system architecture:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:"Isaac Sim (Environment)\n    \u2193\nIsaac ROS (Communication Framework)\n    \u2193\nSensor Integration (Perception)\n    \u2193\nVSLAM (Localization & Mapping)\n    \u2193\nAI Action Planner (Decision Making)\n    \u2193\nNav2 (Path Planning & Execution)\n    \u2193\nRobot Control (Physical Actions)\n"})}),"\n",(0,a.jsx)(e.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,a.jsx)(e.h3,{id:"step-1-environment-setup",children:"Step 1: Environment Setup"}),"\n",(0,a.jsx)(e.p,{children:"Configure Isaac Sim with a realistic indoor environment:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Environment configuration\nenvironment_config = {\n    "scene": "office_building",\n    "lighting": "realistic",\n    "objects": ["desks", "chairs", "people", "obstacles"],\n    "sensors": ["rgb_camera", "lidar", "imu", "depth_camera"]\n}\n\n# Create simulation environment\nsim_env = IsaacSimulationEnvironment()\nsim_env.create_scene(environment_config)\n'})}),"\n",(0,a.jsx)(e.h3,{id:"step-2-sensor-integration-system",children:"Step 2: Sensor Integration System"}),"\n",(0,a.jsx)(e.p,{children:"Implement the sensor fusion system:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# Initialize sensor integration\nsensor_system = SensorIntegration()\nsensor_system.sensor_fusion_enabled = True\nsensor_system.sensor_types = ['rgb_camera', 'lidar', 'imu', 'depth_camera']\nsensor_system.data_frequency = 20  # Hz\n\ndef process_sensor_data():\n    # Collect data from all sensors\n    camera_data = get_camera_data()\n    lidar_data = get_lidar_data()\n    imu_data = get_imu_data()\n    depth_data = get_depth_data()\n\n    # Fuse sensor data\n    fused_perception = sensor_system.integrate_sensors([\n        camera_data, lidar_data, imu_data, depth_data\n    ])\n\n    return fused_perception\n"})}),"\n",(0,a.jsx)(e.h3,{id:"step-3-vslam-integration",children:"Step 3: VSLAM Integration"}),"\n",(0,a.jsx)(e.p,{children:"Connect the VSLAM system for localization:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# Initialize VSLAM system\nvslam_system = VSLAMSystem()\nvslam_system.algorithm_type = \"ORB_SLAM\"\nvslam_system.map_resolution = 0.05\nvslam_system.localization_accuracy = 0.02\n\ndef update_localization(image_frame):\n    # Process frame through VSLAM\n    vslam_result = vslam_system.process_frame(image_frame)\n\n    # Update robot pose estimate\n    robot_pose = vslam_result['pose']\n\n    # Update map\n    map_update = vslam_result['map']\n\n    return robot_pose, map_update\n"})}),"\n",(0,a.jsx)(e.h3,{id:"step-4-nav2-path-planning",children:"Step 4: Nav2 Path Planning"}),"\n",(0,a.jsx)(e.p,{children:"Configure Nav2 for humanoid robot navigation:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Initialize Nav2 planner\nnav2_planner = Nav2PathPlanner()\nnav2_planner.robot_type = "humanoid"\nnav2_planner.costmap_resolution = 0.05\nnav2_planner.inflation_radius = 0.6\n\ndef plan_and_execute_navigation(goal_pose):\n    # Get current robot pose from VSLAM\n    current_pose = get_current_pose()\n\n    # Compute path to goal\n    path = nav2_planner.compute_path(current_pose, goal_pose)\n\n    # Execute path following\n    execution_result = nav2_planner.execute_path(path, controller_config)\n\n    return execution_result\n'})}),"\n",(0,a.jsx)(e.h3,{id:"step-5-ai-action-planning",children:"Step 5: AI Action Planning"}),"\n",(0,a.jsx)(e.p,{children:"Implement the AI decision-making system:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Initialize AI action planner\nai_planner = AIActionPlanner()\nai_planner.decision_algorithm = "hybrid"\nai_planner.planning_horizon = 3.0\nai_planner.safety_constraints = ["avoid_collisions", "maintain_balance"]\n\ndef make_intelligent_decisions(perception_data):\n    # Plan action based on perception\n    action_plan = ai_planner.plan_action(perception_data)\n\n    # Evaluate plan safety\n    evaluation = ai_planner.evaluate_plan(action_plan)\n\n    if evaluation.is_safe:\n        # Execute decision\n        return ai_planner.execute_decision(action_plan.decision)\n    else:\n        # Execute safe fallback\n        return ai_planner.execute_decision("safe_behavior")\n'})}),"\n",(0,a.jsx)(e.h3,{id:"step-6-complete-integration-loop",children:"Step 6: Complete Integration Loop"}),"\n",(0,a.jsx)(e.p,{children:"Combine all systems into a cohesive loop:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'def ai_robot_brain_main_loop():\n    """\n    Main loop for the AI-robot brain system\n    Integrates all components in a coordinated manner\n    """\n    while not goal_reached():\n        # 1. Collect sensor data\n        perception_data = process_sensor_data()\n\n        # 2. Update localization using VSLAM\n        robot_pose, map_update = update_localization(perception_data[\'camera\'])\n\n        # 3. Update Nav2 costmap with new information\n        nav2_planner.update_costmap(perception_data[\'sensors\'])\n\n        # 4. Make intelligent decisions\n        decision = make_intelligent_decisions(perception_data)\n\n        # 5. Plan and execute navigation if needed\n        if decision.requires_navigation:\n            navigation_result = plan_and_execute_navigation(decision.goal_pose)\n\n        # 6. Update system state and continue\n        update_system_state()\n\n        # 7. Sleep for appropriate time\n        time.sleep(1.0 / control_frequency)\n'})}),"\n",(0,a.jsx)(e.h2,{id:"testing-scenarios",children:"Testing Scenarios"}),"\n",(0,a.jsx)(e.p,{children:"Test the complete system with various scenarios:"}),"\n",(0,a.jsx)(e.h3,{id:"scenario-1-simple-navigation",children:"Scenario 1: Simple Navigation"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Robot navigates from start to goal in an empty room"}),"\n",(0,a.jsx)(e.li,{children:"Verify basic path planning and execution"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"scenario-2-obstacle-avoidance",children:"Scenario 2: Obstacle Avoidance"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Robot encounters static obstacles in its path"}),"\n",(0,a.jsx)(e.li,{children:"Verify dynamic path replanning and obstacle avoidance"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"scenario-3-dynamic-environment",children:"Scenario 3: Dynamic Environment"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Moving obstacles (simulated people) in the environment"}),"\n",(0,a.jsx)(e.li,{children:"Verify real-time adaptation to changing conditions"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"scenario-4-complex-navigation",children:"Scenario 4: Complex Navigation"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Multi-room navigation with doorways"}),"\n",(0,a.jsx)(e.li,{children:"Verify map building and localization in complex environments"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"assessment-criteria",children:"Assessment Criteria"}),"\n",(0,a.jsx)(e.p,{children:"Evaluate the AI-robot brain system based on:"}),"\n",(0,a.jsx)(e.h3,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Navigation Success Rate"}),": Percentage of successful goal reaches"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Path Efficiency"}),": Ratio of actual path length to optimal path"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Collision Avoidance"}),": Number of collisions during navigation"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Localization Accuracy"}),": Error in robot position estimation"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"system-integration",children:"System Integration"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Component Coordination"}),": How well different systems work together"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Real-time Performance"}),": Ability to maintain required update rates"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Robustness"}),": System behavior under various conditions"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"ai-decision-quality",children:"AI Decision Quality"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Intelligent Behavior"}),": Appropriate responses to different situations"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Safety Compliance"}),": Adherence to safety constraints"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Goal Achievement"}),": Success in accomplishing assigned tasks"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"mini-exercise",children:"Mini-Exercise"}),"\n",(0,a.jsx)(e.p,{children:"Extend the AI-robot brain system to include a simple task where the robot must find and approach a specific object in the environment. What additional components would you need to add to the system?"}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"This mini-project demonstrates the integration of all concepts from Module 3: Isaac Sim for realistic simulation, Isaac ROS for communication, VSLAM for mapping, Nav2 for navigation, sensor integration for perception, and AI action planning for intelligent decision-making. The AI-robot brain system represents a complete approach to autonomous robot intelligence."}),"\n",(0,a.jsx)(e.p,{children:"Congratulations on completing Module 3! You now understand how to create intelligent, autonomous humanoid robots using the NVIDIA Isaac ecosystem."})]})}function p(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>s,x:()=>r});var t=i(6540);const a={},o=t.createContext(a);function s(n){const e=t.useContext(o);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:s(n.components),t.createElement(o.Provider,{value:e},n.children)}}}]);